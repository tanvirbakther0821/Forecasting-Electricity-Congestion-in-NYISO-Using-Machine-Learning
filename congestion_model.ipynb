{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a785cb64-b21f-411e-ac61-e722ffd2165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "NYC Energy Forecast - Complete Analysis Suite\n",
    "\n",
    "Analyzes energy congestion patterns across:\n",
    "- Central NY (central)\n",
    "- Genese NY (genese)\n",
    "- Long Island (longil)\n",
    "\n",
    "Uses combined datasets with location identifiers for unified analysis.\n",
    "Includes three separate models as per procedure:\n",
    "1. Congestion Model (RTD Zonal Congestion as y)\n",
    "2. Demand Model (TWI Actual Load as y) - NO pricing/congestion features\n",
    "3. LBMP Pricing Model (RTD Zonal LBMP as y) - NO congestion features\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning imports\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "    print(\"âœ… XGBoost loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ XGBoost error: {e}\")\n",
    "    print(\"Using RandomForest instead\")\n",
    "    XGBOOST_AVAILABLE = False\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "\n",
    "class NYCEnergyAnalyzer:\n",
    "    \"\"\"\n",
    "    Comprehensive energy analysis using merged NYC datasets.\n",
    "\n",
    "    Features:\n",
    "    - Loads and combines all merged NYC files\n",
    "    - Creates unified dataset with location-specific weather\n",
    "    - Builds three separate models per procedure:\n",
    "      1. Congestion forecasting\n",
    "      2. Demand forecasting (no pricing/congestion)\n",
    "      3. Price forecasting (no congestion)\n",
    "    - Provides comparative analysis across regions\n",
    "    - Generates comprehensive visualizations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_directory=\".\"):\n",
    "        \"\"\"\n",
    "        Initialize the analyzer.\n",
    "\n",
    "        Args:\n",
    "            data_directory (str): Directory containing merged NYC CSV files\n",
    "        \"\"\"\n",
    "        self.data_directory = data_directory\n",
    "        self.lbmp_data = pd.DataFrame()\n",
    "        self.load_data = pd.DataFrame()\n",
    "        self.weather_data = pd.DataFrame()\n",
    "        self.merged_dataset = pd.DataFrame()\n",
    "        self.congestion_models = {}\n",
    "        self.demand_models = {}\n",
    "        self.price_models = {}\n",
    "        self.location_data = {}\n",
    "\n",
    "    def load_all_merged_files(self):\n",
    "        \"\"\"\n",
    "        Load all merged NYC data files.\n",
    "        Data processing/clean up:\n",
    "        Turn all data into per hour\n",
    "        Next then fill out table accordingly\n",
    "        Using LMBP for the table and it label\n",
    "        Each location has own table\n",
    "        Need data set organized by dollars - congestion times actual load\n",
    "        \"\"\"\n",
    "        print(\"ðŸ“Š Loading All Merged NYC Data Files...\")\n",
    "\n",
    "        # Load LBMP files (all years)\n",
    "        lbmp_files = [\n",
    "            'OASIS_Real_Time_Dispatch_Zonal_LBMP nyc 2021.csv',\n",
    "            'OASIS_Real_Time_Dispatch_Zonal_LBMP nyc 2022.csv',\n",
    "            'OASIS_Real_Time_Dispatch_Zonal_LBMP nyc 2023.csv',\n",
    "            'OASIS_Real_Time_Dispatch_Zonal_LBMP nyc 2024.csv'\n",
    "        ]\n",
    "\n",
    "        lbmp_dataframes = []\n",
    "        for file in lbmp_files:\n",
    "            file_path = os.path.join(self.data_directory, file)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                lbmp_dataframes.append(df)\n",
    "                print(f\"  âœ… Loaded {file}: {len(df)} records\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"  âš ï¸ File not found: {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ Error loading {file}: {e}\")\n",
    "\n",
    "        # Combine LBMP data\n",
    "        if lbmp_dataframes:\n",
    "            self.lbmp_data = pd.concat(lbmp_dataframes, ignore_index=True)\n",
    "            print(f\"  ðŸ“Š Total LBMP records: {len(self.lbmp_data):,}\")\n",
    "\n",
    "        # Load Load files (all years)\n",
    "        load_files = [\n",
    "            'OASIS_Real_Time_Weighted_Integrated_Actual_Load nyc 2021.csv',\n",
    "            'OASIS_Real_Time_Weighted_Integrated_Actual_Load nyc 2022.csv',\n",
    "            'OASIS_Real_Time_Weighted_Integrated_Actual_Load nyc 2023.csv',\n",
    "            'OASIS_Real_Time_Weighted_Integrated_Actual_Load nyc 2024.csv'\n",
    "        ]\n",
    "\n",
    "        load_dataframes = []\n",
    "        for file in load_files:\n",
    "            file_path = os.path.join(self.data_directory, file)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                load_dataframes.append(df)\n",
    "                print(f\"  âœ… Loaded {file}: {len(df)} records\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"  âš ï¸ File not found: {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ Error loading {file}: {e}\")\n",
    "\n",
    "        # Combine Load data\n",
    "        if load_dataframes:\n",
    "            self.load_data = pd.concat(load_dataframes, ignore_index=True)\n",
    "            print(f\"  âš¡ Total Load records: {len(self.load_data):,}\")\n",
    "\n",
    "        # Load Weather data\n",
    "        weather_file = os.path.join(self.data_directory, 'NYC.csv')\n",
    "        try:\n",
    "            self.weather_data = pd.read_csv(weather_file)\n",
    "            print(f\"  âœ… Loaded NYC.csv: {len(self.weather_data)} weather records\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"  âš ï¸ NYC.csv weather file not found!\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Error loading NYC.csv: {e}\")\n",
    "\n",
    "    def show_data_summary(self):\n",
    "        \"\"\"Display summary of loaded data.\"\"\"\n",
    "        print(\"\\nðŸ“‹ DATA SUMMARY\")\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "        if not self.lbmp_data.empty:\n",
    "            print(\"ðŸ“Š LBMP Data:\")\n",
    "            print(f\"   Total records: {len(self.lbmp_data):,}\")\n",
    "            if 'location' in self.lbmp_data.columns:\n",
    "                location_counts = self.lbmp_data['location'].value_counts()\n",
    "                for loc, count in location_counts.items():\n",
    "                    print(f\"   {loc}: {count:,} records\")\n",
    "\n",
    "            # Date range\n",
    "            if 'RTD End Time Stamp' in self.lbmp_data.columns:\n",
    "                self.lbmp_data['RTD End Time Stamp'] = pd.to_datetime(self.lbmp_data['RTD End Time Stamp'])\n",
    "                date_range = f\"{self.lbmp_data['RTD End Time Stamp'].min().date()} to {self.lbmp_data['RTD End Time Stamp'].max().date()}\"\n",
    "                print(f\"   Date range: {date_range}\")\n",
    "\n",
    "        if not self.load_data.empty:\n",
    "            print(\"\\nâš¡ Load Data:\")\n",
    "            print(f\"   Total records: {len(self.load_data):,}\")\n",
    "            if 'location' in self.load_data.columns:\n",
    "                location_counts = self.load_data['location'].value_counts()\n",
    "                for loc, count in location_counts.items():\n",
    "                    print(f\"   {loc}: {count:,} records\")\n",
    "\n",
    "        if not self.weather_data.empty:\n",
    "            print(\"\\nðŸŒ¤ï¸ Weather Data:\")\n",
    "            print(f\"   Total records: {len(self.weather_data):,}\")\n",
    "            if 'location' in self.weather_data.columns:\n",
    "                location_counts = self.weather_data['location'].value_counts()\n",
    "                for loc, count in location_counts.items():\n",
    "                    print(f\"   {loc}: {count:,} records\")\n",
    "\n",
    "    def create_time_features(self, df, timestamp_col):\n",
    "        \"\"\"Create comprehensive time-based features.\"\"\"\n",
    "        df['date'] = df[timestamp_col].dt.date\n",
    "        df['year'] = df[timestamp_col].dt.year\n",
    "        df['month'] = df[timestamp_col].dt.month\n",
    "        df['day'] = df[timestamp_col].dt.day\n",
    "        df['hour'] = df[timestamp_col].dt.hour\n",
    "        df['day_of_week'] = df[timestamp_col].dt.day_name()\n",
    "\n",
    "        # Create binary day-of-week columns\n",
    "        for day in ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']:\n",
    "            df[day] = (df['day_of_week'] == day).astype(int)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def add_holiday_features(self, df):\n",
    "        \"\"\"Add US holiday indicators.\"\"\"\n",
    "        try:\n",
    "            cal = USFederalHolidayCalendar()\n",
    "            start_date = df['date'].min()\n",
    "            end_date = df['date'].max()\n",
    "            holidays = cal.holidays(start=start_date, end=end_date)\n",
    "\n",
    "            df['date_dt'] = pd.to_datetime(df['date'])\n",
    "            df['holiday'] = df['date_dt'].isin(holidays).astype(int)\n",
    "            df = df.drop('date_dt', axis=1)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Could not create holiday features: {e}\")\n",
    "            df['holiday'] = 0\n",
    "\n",
    "        return df\n",
    "\n",
    "    def create_lagged_features_by_location(self, df):\n",
    "        \"\"\"Create lagged features separately for each location.\"\"\"\n",
    "        print(\"ðŸ”„ Creating location-specific lagged features...\")\n",
    "\n",
    "        location_dataframes = []\n",
    "\n",
    "        for location in df['location'].unique():\n",
    "            location_df = df[df['location'] == location].copy()\n",
    "            location_df = location_df.sort_values('RTD End Time Stamp')\n",
    "\n",
    "            # Create lagged features for this location\n",
    "            # LBMP features\n",
    "            location_df['LBMP_yesterday'] = location_df['RTD Zonal LBMP'].shift(24)\n",
    "            location_df['LBMP_7_days_ago'] = location_df['RTD Zonal LBMP'].shift(168)\n",
    "            location_df['LBMP_1_hour_ago'] = location_df['RTD Zonal LBMP'].shift(1)\n",
    "\n",
    "            # Price rolling statistics\n",
    "            location_df['price_24h_avg'] = location_df['RTD Zonal LBMP'].rolling(window=24, min_periods=1).mean()\n",
    "            location_df['price_7d_avg'] = location_df['RTD Zonal LBMP'].rolling(window=168, min_periods=1).mean()\n",
    "            location_df['price_24h_std'] = location_df['RTD Zonal LBMP'].rolling(window=24, min_periods=1).std()\n",
    "            location_df['price_volatility_24h'] = location_df['RTD Zonal LBMP'].rolling(window=24, min_periods=1).std()\n",
    "            location_df['price_range_24h'] = (location_df['RTD Zonal LBMP'].rolling(window=24, min_periods=1).max() -\n",
    "                                            location_df['RTD Zonal LBMP'].rolling(window=24, min_periods=1).min())\n",
    "\n",
    "            # Price momentum\n",
    "            location_df['price_momentum_1h'] = location_df['RTD Zonal LBMP'] - location_df['LBMP_1_hour_ago']\n",
    "            location_df['price_momentum_24h'] = location_df['RTD Zonal LBMP'] - location_df['LBMP_yesterday']\n",
    "\n",
    "            # Congestion features\n",
    "            location_df['congestion_yesterday'] = location_df['RTD Zonal Congestion'].shift(24)\n",
    "            location_df['congestion_7_days_ago'] = location_df['RTD Zonal Congestion'].shift(168)\n",
    "\n",
    "            location_dataframes.append(location_df)\n",
    "\n",
    "        # Combine all locations back together\n",
    "        combined_df = pd.concat(location_dataframes, ignore_index=True)\n",
    "        combined_df = combined_df.sort_values(['location', 'RTD End Time Stamp'])\n",
    "\n",
    "        print(\"âœ… Lagged features created for all locations\")\n",
    "        return combined_df\n",
    "\n",
    "    def create_demand_lagged_features_by_location(self, df):\n",
    "        \"\"\"Create lagged demand features separately for each location.\"\"\"\n",
    "        print(\"ðŸ”„ Creating location-specific lagged demand features...\")\n",
    "\n",
    "        location_dataframes = []\n",
    "\n",
    "        for location in df['location'].unique():\n",
    "            location_df = df[df['location'] == location].copy()\n",
    "            location_df = location_df.sort_values('Eastern Date Hour')\n",
    "\n",
    "            # Create lagged demand features for this location\n",
    "            location_df['demand_yesterday'] = location_df['TWI Actual Load'].shift(24)\n",
    "            location_df['demand_7_days_ago'] = location_df['TWI Actual Load'].shift(168)\n",
    "            location_df['demand_1_hour_ago'] = location_df['TWI Actual Load'].shift(1)\n",
    "\n",
    "            # Create rolling averages\n",
    "            location_df['demand_24h_avg'] = location_df['TWI Actual Load'].rolling(window=24, min_periods=1).mean()\n",
    "            location_df['demand_7d_avg'] = location_df['TWI Actual Load'].rolling(window=168, min_periods=1).mean()\n",
    "\n",
    "            location_dataframes.append(location_df)\n",
    "\n",
    "        # Combine all locations back together\n",
    "        combined_df = pd.concat(location_dataframes, ignore_index=True)\n",
    "        combined_df = combined_df.sort_values(['location', 'Eastern Date Hour'])\n",
    "\n",
    "        print(\"âœ… Lagged demand features created for all locations\")\n",
    "        return combined_df\n",
    "\n",
    "    def merge_all_data(self):\n",
    "        \"\"\"Merge LBMP, Load, and Weather data with location-specific weather mapping.\"\"\"\n",
    "        print(\"\\nðŸ”— Merging All NYC Data...\")\n",
    "\n",
    "        if self.lbmp_data.empty:\n",
    "            print(\"âŒ No LBMP data available!\")\n",
    "            return\n",
    "\n",
    "        # Start with LBMP data\n",
    "        print(\"  Processing LBMP data...\")\n",
    "        merged = self.lbmp_data.copy()\n",
    "        merged['RTD End Time Stamp'] = pd.to_datetime(merged['RTD End Time Stamp'])\n",
    "        merged = self.create_time_features(merged, 'RTD End Time Stamp')\n",
    "\n",
    "        # Create lagged features by location\n",
    "        merged = self.create_lagged_features_by_location(merged)\n",
    "\n",
    "        # Group by location, year, month, day, hour for hourly averages\n",
    "        print(\"  Creating hourly averages...\")\n",
    "        hourly_lbmp = merged.groupby(['location', 'year', 'month', 'day', 'hour']).agg({\n",
    "            'RTD End Time Stamp': 'first',\n",
    "            'RTD Zonal LBMP': 'mean',\n",
    "            'RTD Zonal Losses': 'mean',\n",
    "            'RTD Zonal Congestion': 'mean',\n",
    "            'LBMP_yesterday': 'mean',\n",
    "            'LBMP_7_days_ago': 'mean',\n",
    "            'LBMP_1_hour_ago': 'mean',\n",
    "            'price_24h_avg': 'mean',\n",
    "            'price_7d_avg': 'mean',\n",
    "            'price_24h_std': 'mean',\n",
    "            'price_volatility_24h': 'mean',\n",
    "            'price_range_24h': 'mean',\n",
    "            'price_momentum_1h': 'mean',\n",
    "            'price_momentum_24h': 'mean',\n",
    "            'congestion_yesterday': 'mean',\n",
    "            'congestion_7_days_ago': 'mean',\n",
    "            'date': 'first',\n",
    "            'day_of_week': 'first',\n",
    "            'Sunday': 'first', 'Monday': 'first', 'Tuesday': 'first', 'Wednesday': 'first',\n",
    "            'Thursday': 'first', 'Friday': 'first', 'Saturday': 'first'\n",
    "        }).reset_index()\n",
    "\n",
    "        # Add holiday features\n",
    "        hourly_lbmp = self.add_holiday_features(hourly_lbmp)\n",
    "\n",
    "        # Merge with Load data\n",
    "        if not self.load_data.empty:\n",
    "            print(\"  Merging Load data...\")\n",
    "            load_data = self.load_data.copy()\n",
    "            load_data['Eastern Date Hour'] = pd.to_datetime(load_data['Eastern Date Hour'])\n",
    "            load_data = self.create_time_features(load_data, 'Eastern Date Hour')\n",
    "\n",
    "            # Create demand lagged features\n",
    "            load_data = self.create_demand_lagged_features_by_location(load_data)\n",
    "\n",
    "            # Group load data by location and hour\n",
    "            hourly_load = load_data.groupby(['location', 'year', 'month', 'day', 'hour']).agg({\n",
    "                'TWI Actual Load': 'mean',\n",
    "                'demand_yesterday': 'mean',\n",
    "                'demand_7_days_ago': 'mean',\n",
    "                'demand_1_hour_ago': 'mean',\n",
    "                'demand_24h_avg': 'mean',\n",
    "                'demand_7d_avg': 'mean'\n",
    "            }).reset_index()\n",
    "\n",
    "            # Merge with LBMP data\n",
    "            hourly_lbmp = pd.merge(hourly_lbmp, hourly_load,\n",
    "                                 on=['location', 'year', 'month', 'day', 'hour'], how='left')\n",
    "            print(\"    âœ… Load data merged\")\n",
    "\n",
    "        # Merge with Weather data (location-specific)\n",
    "        if not self.weather_data.empty:\n",
    "            print(\"  Merging Weather data...\")\n",
    "            weather_data = self.weather_data.copy()\n",
    "            weather_data['valid'] = pd.to_datetime(weather_data['valid'])\n",
    "            weather_data = self.create_time_features(weather_data, 'valid')\n",
    "\n",
    "            # Group weather data by location and hour\n",
    "            hourly_weather = weather_data.groupby(['location', 'year', 'month', 'day', 'hour']).agg({\n",
    "                'tmpf': 'mean',\n",
    "                'relh': 'mean'\n",
    "            }).reset_index()\n",
    "\n",
    "            # Rename weather columns for clarity\n",
    "            hourly_weather = hourly_weather.rename(columns={'tmpf': 'temperature_F', 'relh': 'humidity_pct'})\n",
    "\n",
    "            # Merge with main data\n",
    "            hourly_lbmp = pd.merge(hourly_lbmp, hourly_weather,\n",
    "                                 on=['location', 'year', 'month', 'day', 'hour'], how='left')\n",
    "            print(\"    âœ… Weather data merged\")\n",
    "\n",
    "        # Create additional features\n",
    "        if 'RTD Zonal LBMP' in hourly_lbmp.columns and 'TWI Actual Load' in hourly_lbmp.columns:\n",
    "            hourly_lbmp['price_demand'] = hourly_lbmp['RTD Zonal LBMP'] * hourly_lbmp['TWI Actual Load']\n",
    "            hourly_lbmp['price_per_MW'] = hourly_lbmp['RTD Zonal LBMP'] / (hourly_lbmp['TWI Actual Load'] + 1)\n",
    "            hourly_lbmp['demand_price_ratio'] = hourly_lbmp['TWI Actual Load'] / (hourly_lbmp['RTD Zonal LBMP'] + 1)\n",
    "            print(\"    âœ… Additional features created\")\n",
    "\n",
    "        self.merged_dataset = hourly_lbmp\n",
    "        print(f\"âœ… Final merged dataset: {len(self.merged_dataset)} records\")\n",
    "\n",
    "        # Split data by location for individual analysis\n",
    "        for location in self.merged_dataset['location'].unique():\n",
    "            self.location_data[location] = self.merged_dataset[self.merged_dataset['location'] == location].copy()\n",
    "            print(f\"   {location}: {len(self.location_data[location]):,} records\")\n",
    "\n",
    "    def prepare_congestion_model_data(self, data, include_location_features=False):\n",
    "        \"\"\"Prepare data for congestion forecasting.\"\"\"\n",
    "\n",
    "        # Base features\n",
    "        feature_columns = [\n",
    "            'month', 'day', 'hour',\n",
    "            'Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday',\n",
    "            'holiday'\n",
    "        ]\n",
    "\n",
    "        # Add location dummy variables if requested\n",
    "        if include_location_features and 'location' in data.columns:\n",
    "            location_dummies = pd.get_dummies(data['location'], prefix='location')\n",
    "            data = pd.concat([data, location_dummies], axis=1)\n",
    "            feature_columns.extend(location_dummies.columns.tolist())\n",
    "\n",
    "        # Add lagged congestion features\n",
    "        congestion_features = ['congestion_yesterday', 'congestion_7_days_ago']\n",
    "        for feature in congestion_features:\n",
    "            if feature in data.columns:\n",
    "                feature_columns.append(feature)\n",
    "\n",
    "        # Add demand features\n",
    "        if 'TWI Actual Load' in data.columns:\n",
    "            feature_columns.append('TWI Actual Load')\n",
    "\n",
    "        # Add weather features\n",
    "        weather_features = ['temperature_F', 'humidity_pct']\n",
    "        for feature in weather_features:\n",
    "            if feature in data.columns:\n",
    "                feature_columns.append(feature)\n",
    "\n",
    "        # Add price features\n",
    "        price_features = ['RTD Zonal LBMP', 'RTD Zonal Losses']\n",
    "        for feature in price_features:\n",
    "            if feature in data.columns:\n",
    "                feature_columns.append(feature)\n",
    "\n",
    "        # Create model dataset\n",
    "        model_data = data[feature_columns + ['RTD Zonal Congestion']].copy()\n",
    "        model_data = model_data.dropna(subset=['RTD Zonal Congestion'])\n",
    "\n",
    "        # Fill missing values\n",
    "        for col in feature_columns:\n",
    "            if col in model_data.columns and model_data[col].isnull().sum() > 0:\n",
    "                model_data[col] = model_data[col].fillna(model_data[col].median())\n",
    "\n",
    "        X = model_data[feature_columns]\n",
    "        y = model_data['RTD Zonal Congestion']\n",
    "\n",
    "        return X, y, feature_columns\n",
    "\n",
    "    def prepare_demand_model_data(self, data, include_location_features=False):\n",
    "        \"\"\"\n",
    "        Prepare data for demand forecasting models.\n",
    "        PROCEDURE COMPLIANT: No pricing or congestion features allowed.\n",
    "        \"\"\"\n",
    "\n",
    "        # Base features for demand forecasting\n",
    "        feature_columns = [\n",
    "            'month', 'day', 'hour',\n",
    "            'Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday',\n",
    "            'holiday'\n",
    "        ]\n",
    "\n",
    "        # Add location dummy variables if requested\n",
    "        if include_location_features and 'location' in data.columns:\n",
    "            location_dummies = pd.get_dummies(data['location'], prefix='location')\n",
    "            data = pd.concat([data, location_dummies], axis=1)\n",
    "            feature_columns.extend(location_dummies.columns.tolist())\n",
    "\n",
    "        # Add lagged demand features (key for demand forecasting)\n",
    "        lagged_features = ['demand_yesterday', 'demand_7_days_ago', 'demand_1_hour_ago',\n",
    "                          'demand_24h_avg', 'demand_7d_avg']\n",
    "\n",
    "        for feature in lagged_features:\n",
    "            if feature in data.columns:\n",
    "                feature_columns.append(feature)\n",
    "\n",
    "        # Add weather features ONLY (temperature strongly affects electricity demand)\n",
    "        weather_features = ['temperature_F', 'humidity_pct']\n",
    "\n",
    "        for feature in weather_features:\n",
    "            if feature in data.columns:\n",
    "                feature_columns.append(feature)\n",
    "\n",
    "        # âŒ REMOVED: Price and congestion features per procedure\n",
    "        # price_features = ['RTD Zonal LBMP', 'RTD Zonal Losses', 'RTD Zonal Congestion']\n",
    "\n",
    "        # Create model dataset\n",
    "        model_data = data[feature_columns + ['TWI Actual Load']].copy()\n",
    "        model_data = model_data.dropna(subset=['TWI Actual Load'])\n",
    "\n",
    "        # Fill missing values\n",
    "        for col in feature_columns:\n",
    "            if col in model_data.columns and model_data[col].isnull().sum() > 0:\n",
    "                model_data[col] = model_data[col].fillna(model_data[col].median())\n",
    "\n",
    "        X = model_data[feature_columns]\n",
    "        y = model_data['TWI Actual Load']\n",
    "\n",
    "        return X, y, feature_columns\n",
    "\n",
    "    def prepare_price_model_data(self, data, include_location_features=False):\n",
    "        \"\"\"\n",
    "        Prepare data for price forecasting models.\n",
    "        PROCEDURE COMPLIANT: No congestion features allowed.\n",
    "        \"\"\"\n",
    "\n",
    "        # Base features for price forecasting\n",
    "        feature_columns = [\n",
    "            'month', 'day', 'hour',\n",
    "            'Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday',\n",
    "            'holiday'\n",
    "        ]\n",
    "\n",
    "        # Add location dummy variables if requested\n",
    "        if include_location_features and 'location' in data.columns:\n",
    "            location_dummies = pd.get_dummies(data['location'], prefix='location')\n",
    "            data = pd.concat([data, location_dummies], axis=1)\n",
    "            feature_columns.extend(location_dummies.columns.tolist())\n",
    "\n",
    "        # Add lagged price features (key for price forecasting)\n",
    "        price_features = ['LBMP_yesterday', 'LBMP_7_days_ago', 'LBMP_1_hour_ago',\n",
    "                         'price_24h_avg', 'price_7d_avg', 'price_24h_std',\n",
    "                         'price_volatility_24h', 'price_range_24h',\n",
    "                         'price_momentum_1h', 'price_momentum_24h']\n",
    "\n",
    "        for feature in price_features:\n",
    "            if feature in data.columns:\n",
    "                feature_columns.append(feature)\n",
    "\n",
    "        # Add demand features (demand strongly affects price)\n",
    "        demand_features = ['TWI Actual Load']\n",
    "\n",
    "        for feature in demand_features:\n",
    "            if feature in data.columns:\n",
    "                feature_columns.append(feature)\n",
    "\n",
    "        # Add ONLY losses component (not congestion per procedure)\n",
    "        lbmp_components = ['RTD Zonal Losses']  # âœ… REMOVED CONGESTION\n",
    "\n",
    "        for feature in lbmp_components:\n",
    "            if feature in data.columns:\n",
    "                feature_columns.append(feature)\n",
    "\n",
    "        # Add weather features (affects demand and hence price)\n",
    "        weather_features = ['temperature_F', 'humidity_pct']\n",
    "\n",
    "        for feature in weather_features:\n",
    "            if feature in data.columns:\n",
    "                feature_columns.append(feature)\n",
    "\n",
    "        # Add interaction features\n",
    "        interaction_features = ['price_per_MW', 'demand_price_ratio']\n",
    "\n",
    "        for feature in interaction_features:\n",
    "            if feature in data.columns:\n",
    "                feature_columns.append(feature)\n",
    "\n",
    "        # Create model dataset\n",
    "        model_data = data[feature_columns + ['RTD Zonal LBMP']].copy()\n",
    "        model_data = model_data.dropna(subset=['RTD Zonal LBMP'])\n",
    "\n",
    "        # Fill missing values\n",
    "        for col in feature_columns:\n",
    "            if col in model_data.columns and model_data[col].isnull().sum() > 0:\n",
    "                model_data[col] = model_data[col].fillna(model_data[col].median())\n",
    "\n",
    "        X = model_data[feature_columns]\n",
    "        y = model_data['RTD Zonal LBMP']\n",
    "\n",
    "        return X, y, feature_columns\n",
    "\n",
    "    def train_all_models(self):\n",
    "        \"\"\"Train all three models as per procedure.\"\"\"\n",
    "        print(\"\\nðŸŽ¯ Training All Models Per Procedure...\")\n",
    "\n",
    "        # Train individual location models\n",
    "        self.train_congestion_models()\n",
    "        self.train_demand_models()\n",
    "        self.train_price_models()\n",
    "\n",
    "        # Train combined models using all regions together\n",
    "        self.train_combined_congestion_model()\n",
    "        self.train_combined_demand_model()\n",
    "        self.train_combined_price_model()\n",
    "\n",
    "    def train_congestion_models(self):\n",
    "        \"\"\"Train congestion forecasting models for each location.\"\"\"\n",
    "        print(\"\\nðŸ“Š Training Congestion Models...\")\n",
    "\n",
    "        for location, data in self.location_data.items():\n",
    "            if len(data) < 100:\n",
    "                print(f\"  âš ï¸ Skipping {location}: insufficient data ({len(data)} records)\")\n",
    "                continue\n",
    "\n",
    "            print(f\"  Training congestion model for {location}...\")\n",
    "\n",
    "            # Prepare data\n",
    "            X, y, feature_names = self.prepare_congestion_model_data(data, include_location_features=False)\n",
    "\n",
    "            if len(X) == 0:\n",
    "                print(f\"    âŒ No valid data for {location}\")\n",
    "                continue\n",
    "\n",
    "            # Split data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "            # Train model\n",
    "            if XGBOOST_AVAILABLE:\n",
    "                try:\n",
    "                    model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "                    model_type = \"XGBoost\"\n",
    "                except:\n",
    "                    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "                    model_type = \"RandomForest\"\n",
    "            else:\n",
    "                model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "                model_type = \"RandomForest\"\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Evaluate\n",
    "            y_pred = model.predict(X_test)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "            # Store results\n",
    "            self.congestion_models[location] = {\n",
    "                'model': model,\n",
    "                'model_type': model_type,\n",
    "                'X_test': X_test,\n",
    "                'y_test': y_test,\n",
    "                'y_pred': y_pred,\n",
    "                'feature_names': feature_names,\n",
    "                'metrics': {'r2': r2, 'mape': mape, 'rmse': rmse}\n",
    "            }\n",
    "\n",
    "            print(f\"    ðŸ“Š {location} {model_type} Congestion Results:\")\n",
    "            print(f\"       RÂ²: {r2:.4f}\")\n",
    "            print(f\"       MAPE: {mape:.4f}\")\n",
    "            print(f\"       RMSE: {rmse:.4f}\")\n",
    "\n",
    "    def train_demand_models(self):\n",
    "        \"\"\"Train demand forecasting models for each location.\"\"\"\n",
    "        print(\"\\nâš¡ Training Demand Models (No Pricing/Congestion)...\")\n",
    "\n",
    "        for location, data in self.location_data.items():\n",
    "            if len(data) < 100:\n",
    "                print(f\"  âš ï¸ Skipping {location}: insufficient data ({len(data)} records)\")\n",
    "                continue\n",
    "\n",
    "            print(f\"  Training demand model for {location}...\")\n",
    "\n",
    "            # Prepare data\n",
    "            X, y, feature_names = self.prepare_demand_model_data(data, include_location_features=False)\n",
    "\n",
    "            if len(X) == 0:\n",
    "                print(f\"    âŒ No valid data for {location}\")\n",
    "                continue\n",
    "\n",
    "            # Split data (temporal split for demand forecasting)\n",
    "            split_idx = int(len(X) * 0.8)\n",
    "            X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "            y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "            # Train model\n",
    "            if XGBOOST_AVAILABLE:\n",
    "                try:\n",
    "                    model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "                    model_type = \"XGBoost\"\n",
    "                except:\n",
    "                    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "                    model_type = \"RandomForest\"\n",
    "            else:\n",
    "                model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "                model_type = \"RandomForest\"\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Evaluate\n",
    "            y_pred = model.predict(X_test)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "            # Store results\n",
    "            self.demand_models[location] = {\n",
    "                'model': model,\n",
    "                'model_type': model_type,\n",
    "                'X_test': X_test,\n",
    "                'y_test': y_test,\n",
    "                'y_pred': y_pred,\n",
    "                'feature_names': feature_names,\n",
    "                'metrics': {'r2': r2, 'mape': mape, 'rmse': rmse}\n",
    "            }\n",
    "\n",
    "            print(f\"    âš¡ {location} {model_type} Demand Results:\")\n",
    "            print(f\"       RÂ²: {r2:.4f}\")\n",
    "            print(f\"       MAPE: {mape:.4f}\")\n",
    "            print(f\"       RMSE: {rmse:.2f} MW\")\n",
    "\n",
    "    def train_price_models(self):\n",
    "        \"\"\"Train price forecasting models for each location.\"\"\"\n",
    "        print(\"\\nðŸ’° Training Price Models (No Congestion)...\")\n",
    "\n",
    "        for location, data in self.location_data.items():\n",
    "            if len(data) < 100:\n",
    "                print(f\"  âš ï¸ Skipping {location}: insufficient data ({len(data)} records)\")\n",
    "                continue\n",
    "\n",
    "            print(f\"  Training price model for {location}...\")\n",
    "\n",
    "            # Prepare data\n",
    "            X, y, feature_names = self.prepare_price_model_data(data, include_location_features=False)\n",
    "\n",
    "            if len(X) == 0:\n",
    "                print(f\"    âŒ No valid data for {location}\")\n",
    "                continue\n",
    "\n",
    "            # Split data (temporal split for price forecasting)\n",
    "            split_idx = int(len(X) * 0.8)\n",
    "            X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "            y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "            # Train model\n",
    "            if XGBOOST_AVAILABLE:\n",
    "                try:\n",
    "                    model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "                    model_type = \"XGBoost\"\n",
    "                except:\n",
    "                    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "                    model_type = \"RandomForest\"\n",
    "            else:\n",
    "                model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "                model_type = \"RandomForest\"\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Evaluate\n",
    "            y_pred = model.predict(X_test)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "            # Store results\n",
    "            self.price_models[location] = {\n",
    "                'model': model,\n",
    "                'model_type': model_type,\n",
    "                'X_test': X_test,\n",
    "                'y_test': y_test,\n",
    "                'y_pred': y_pred,\n",
    "                'feature_names': feature_names,\n",
    "                'metrics': {'r2': r2, 'mape': mape, 'rmse': rmse}\n",
    "            }\n",
    "\n",
    "            print(f\"    ðŸ’° {location} {model_type} Price Results:\")\n",
    "            print(f\"       RÂ²: {r2:.4f}\")\n",
    "            print(f\"       MAPE: {mape:.4f}\")\n",
    "            print(f\"       RMSE: ${rmse:.2f}/MWh\")\n",
    "\n",
    "    def train_combined_congestion_model(self):\n",
    "        \"\"\"Train a combined congestion model using all locations.\"\"\"\n",
    "        print(\"\\nðŸŒ Training Combined Congestion Model...\")\n",
    "\n",
    "        # Prepare combined data with location features\n",
    "        X, y, feature_names = self.prepare_congestion_model_data(self.merged_dataset, include_location_features=True)\n",
    "\n",
    "        if len(X) == 0:\n",
    "            print(\"âŒ No valid data for combined congestion model\")\n",
    "            return\n",
    "\n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Train model\n",
    "        if XGBOOST_AVAILABLE:\n",
    "            try:\n",
    "                model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "                model_type = \"XGBoost\"\n",
    "            except:\n",
    "                model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "                model_type = \"RandomForest\"\n",
    "        else:\n",
    "            model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            model_type = \"RandomForest\"\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate\n",
    "        y_pred = model.predict(X_test)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "        # Store results\n",
    "        self.congestion_models['combined'] = {\n",
    "            'model': model,\n",
    "            'model_type': model_type,\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "            'y_pred': y_pred,\n",
    "            'feature_names': feature_names,\n",
    "            'metrics': {'r2': r2, 'mape': mape, 'rmse': rmse}\n",
    "        }\n",
    "\n",
    "        print(f\"ðŸ“Š Combined {model_type} Congestion Results:\")\n",
    "        print(f\"   RÂ²: {r2:.4f}\")\n",
    "        print(f\"   MAPE: {mape:.4f}\")\n",
    "        print(f\"   RMSE: {rmse:.4f}\")\n",
    "\n",
    "    def train_combined_demand_model(self):\n",
    "        \"\"\"Train a combined demand model using all locations.\"\"\"\n",
    "        print(\"\\nðŸŒ Training Combined Demand Model (No Pricing/Congestion)...\")\n",
    "\n",
    "        # Prepare combined data with location features\n",
    "        X, y, feature_names = self.prepare_demand_model_data(self.merged_dataset, include_location_features=True)\n",
    "\n",
    "        if len(X) == 0:\n",
    "            print(\"âŒ No valid data for combined demand model\")\n",
    "            return\n",
    "\n",
    "        # Split data (temporal split for demand forecasting)\n",
    "        split_idx = int(len(X) * 0.8)\n",
    "        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "        # Train model\n",
    "        if XGBOOST_AVAILABLE:\n",
    "            try:\n",
    "                model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "                model_type = \"XGBoost\"\n",
    "            except:\n",
    "                model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "                model_type = \"RandomForest\"\n",
    "        else:\n",
    "            model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            model_type = \"RandomForest\"\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate\n",
    "        y_pred = model.predict(X_test)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "        # Store results\n",
    "        self.demand_models['combined'] = {\n",
    "            'model': model,\n",
    "            'model_type': model_type,\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "            'y_pred': y_pred,\n",
    "            'feature_names': feature_names,\n",
    "            'metrics': {'r2': r2, 'mape': mape, 'rmse': rmse}\n",
    "        }\n",
    "\n",
    "        print(f\"ðŸ“Š Combined {model_type} Demand Results:\")\n",
    "        print(f\"   RÂ²: {r2:.4f}\")\n",
    "        print(f\"   MAPE: {mape:.4f}\")\n",
    "        print(f\"   RMSE: {rmse:.2f} MW\")\n",
    "\n",
    "    def train_combined_price_model(self):\n",
    "        \"\"\"Train a combined price model using all locations.\"\"\"\n",
    "        print(\"\\nðŸŒ Training Combined Price Model (No Congestion)...\")\n",
    "\n",
    "        # Prepare combined data with location features\n",
    "        X, y, feature_names = self.prepare_price_model_data(self.merged_dataset, include_location_features=True)\n",
    "\n",
    "        if len(X) == 0:\n",
    "            print(\"âŒ No valid data for combined price model\")\n",
    "            return\n",
    "\n",
    "        # Split data (temporal split for price forecasting)\n",
    "        split_idx = int(len(X) * 0.8)\n",
    "        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "        # Train model\n",
    "        if XGBOOST_AVAILABLE:\n",
    "            try:\n",
    "                model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "                model_type = \"XGBoost\"\n",
    "            except:\n",
    "                model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "                model_type = \"RandomForest\"\n",
    "        else:\n",
    "            model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            model_type = \"RandomForest\"\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate\n",
    "        y_pred = model.predict(X_test)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "        # Store results\n",
    "        self.price_models['combined'] = {\n",
    "            'model': model,\n",
    "            'model_type': model_type,\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "            'y_pred': y_pred,\n",
    "            'feature_names': feature_names,\n",
    "            'metrics': {'r2': r2, 'mape': mape, 'rmse': rmse}\n",
    "        }\n",
    "\n",
    "        print(f\"ðŸ“Š Combined {model_type} Price Results:\")\n",
    "        print(f\"   RÂ²: {r2:.4f}\")\n",
    "        print(f\"   MAPE: {mape:.4f}\")\n",
    "        print(f\"   RMSE: ${rmse:.2f}/MWh\")\n",
    "\n",
    "    def create_comprehensive_visualizations(self):\n",
    "        \"\"\"\n",
    "        Create comprehensive visualizations for all analyses.\n",
    "        Outputs of models:\n",
    "        * X y graph for pricing, demand, and congestion\n",
    "        * X is original price, demand, congestion\n",
    "        * Y is predict price, demand, congestion\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸ“ˆ Creating Comprehensive Visualizations...\")\n",
    "\n",
    "        # 1. REGIONAL COMPARISON DASHBOARD\n",
    "        self.create_regional_comparison_dashboard()\n",
    "\n",
    "        # 2. ACTUAL VS PREDICTED PLOTS (as per procedure)\n",
    "        self.create_actual_vs_predicted_plots()\n",
    "\n",
    "        # 3. FEATURE IMPORTANCE COMPARISON\n",
    "        self.create_feature_importance_comparison()\n",
    "\n",
    "        # 4. TIME SERIES ANALYSIS\n",
    "        self.create_time_series_analysis()\n",
    "\n",
    "    def create_regional_comparison_dashboard(self):\n",
    "        \"\"\"Create regional comparison dashboard.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle('NYC Energy Market Regional Comparison Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "        if self.merged_dataset.empty:\n",
    "            return\n",
    "\n",
    "        # 1. Average Congestion by Location\n",
    "        congestion_by_location = self.merged_dataset.groupby('location')['RTD Zonal Congestion'].mean()\n",
    "        axes[0, 0].bar(congestion_by_location.index, congestion_by_location.values,\n",
    "                      color=['lightblue', 'lightgreen', 'lightcoral'])\n",
    "        axes[0, 0].set_title('Average Congestion by Region')\n",
    "        axes[0, 0].set_ylabel('Average Congestion ($/MWh)')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        # 2. Average Demand by Location\n",
    "        if 'TWI Actual Load' in self.merged_dataset.columns:\n",
    "            demand_by_location = self.merged_dataset.groupby('location')['TWI Actual Load'].mean()\n",
    "            axes[0, 1].bar(demand_by_location.index, demand_by_location.values,\n",
    "                          color=['lightblue', 'lightgreen', 'lightcoral'])\n",
    "            axes[0, 1].set_title('Average Demand by Region')\n",
    "            axes[0, 1].set_ylabel('Average Demand (MW)')\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "        # 3. Average Price by Location\n",
    "        price_by_location = self.merged_dataset.groupby('location')['RTD Zonal LBMP'].mean()\n",
    "        axes[0, 2].bar(price_by_location.index, price_by_location.values,\n",
    "                      color=['lightblue', 'lightgreen', 'lightcoral'])\n",
    "        axes[0, 2].set_title('Average Price by Region')\n",
    "        axes[0, 2].set_ylabel('Average Price ($/MWh)')\n",
    "        axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "        # 4. Congestion Model Performance\n",
    "        if self.congestion_models:\n",
    "            locations = [loc if loc != 'combined' else 'All Combined' for loc in self.congestion_models.keys()]\n",
    "            r2_scores = [self.congestion_models[loc]['metrics']['r2'] for loc in self.congestion_models.keys()]\n",
    "            colors = ['lightblue', 'lightgreen', 'lightcoral', 'gold'][:len(locations)]\n",
    "            axes[1, 0].bar(locations, r2_scores, color=colors)\n",
    "            axes[1, 0].set_title('Congestion Model Performance (RÂ²)')\n",
    "            axes[1, 0].set_ylabel('RÂ² Score')\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "            axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # 5. Demand Model Performance\n",
    "        if self.demand_models:\n",
    "            locations = [loc if loc != 'combined' else 'All Combined' for loc in self.demand_models.keys()]\n",
    "            r2_scores = [self.demand_models[loc]['metrics']['r2'] for loc in self.demand_models.keys()]\n",
    "            colors = ['lightblue', 'lightgreen', 'lightcoral', 'gold'][:len(locations)]\n",
    "            axes[1, 1].bar(locations, r2_scores, color=colors)\n",
    "            axes[1, 1].set_title('Demand Model Performance (RÂ²)')\n",
    "            axes[1, 1].set_ylabel('RÂ² Score')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "            axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # 6. Price Model Performance\n",
    "        if self.price_models:\n",
    "            locations = [loc if loc != 'combined' else 'All Combined' for loc in self.price_models.keys()]\n",
    "            r2_scores = [self.price_models[loc]['metrics']['r2'] for loc in self.price_models.keys()]\n",
    "            colors = ['lightblue', 'lightgreen', 'lightcoral', 'gold'][:len(locations)]\n",
    "            axes[1, 2].bar(locations, r2_scores, color=colors)\n",
    "            axes[1, 2].set_title('Price Model Performance (RÂ²)')\n",
    "            axes[1, 2].set_ylabel('RÂ² Score')\n",
    "            axes[1, 2].grid(True, alpha=0.3)\n",
    "            axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def create_actual_vs_predicted_plots(self):\n",
    "        \"\"\"\n",
    "        Create actual vs predicted plots as per procedure.\n",
    "        X y graph for pricing, demand, and congestion\n",
    "        X is original price, demand, congestion\n",
    "        Y is predict price, demand, congestion\n",
    "        \"\"\"\n",
    "        print(\"ðŸ“Š Creating Actual vs Predicted Plots (Per Procedure)...\")\n",
    "\n",
    "        # Determine number of locations (including combined models)\n",
    "        all_locations = set()\n",
    "        if self.congestion_models:\n",
    "            all_locations.update(self.congestion_models.keys())\n",
    "        if self.demand_models:\n",
    "            all_locations.update(self.demand_models.keys())\n",
    "        if self.price_models:\n",
    "            all_locations.update(self.price_models.keys())\n",
    "\n",
    "        # Separate individual locations from combined\n",
    "        individual_locations = [loc for loc in all_locations if loc != 'combined']\n",
    "        has_combined = 'combined' in all_locations\n",
    "\n",
    "        n_cols = len(individual_locations) + (1 if has_combined else 0)\n",
    "\n",
    "        if n_cols == 0:\n",
    "            print(\"No models available for plotting.\")\n",
    "            return\n",
    "\n",
    "        # Create plots for each model type\n",
    "        fig, axes = plt.subplots(3, n_cols, figsize=(5*n_cols, 15))\n",
    "        if n_cols == 1:\n",
    "            axes = axes.reshape(3, 1)\n",
    "\n",
    "        fig.suptitle('Actual vs Predicted: Congestion, Demand, and Price Models (Individual + Combined)', fontsize=16, fontweight='bold')\n",
    "\n",
    "        plot_locations = individual_locations + (['combined'] if has_combined else [])\n",
    "\n",
    "        for col, location in enumerate(plot_locations):\n",
    "            location_title = location.title() if location != 'combined' else 'All Regions Combined'\n",
    "\n",
    "            # 1. Congestion Model Plot\n",
    "            if location in self.congestion_models:\n",
    "                model_info = self.congestion_models[location]\n",
    "                y_test = model_info['y_test']\n",
    "                y_pred = model_info['y_pred']\n",
    "                r2 = model_info['metrics']['r2']\n",
    "\n",
    "                axes[0, col].scatter(y_test, y_pred, alpha=0.6, s=20, color='red')\n",
    "                min_val, max_val = min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())\n",
    "                axes[0, col].plot([min_val, max_val], [min_val, max_val], 'k--', lw=2, label='Perfect Prediction')\n",
    "                axes[0, col].set_xlabel('Actual Congestion ($/MWh)')\n",
    "                axes[0, col].set_ylabel('Predicted Congestion ($/MWh)')\n",
    "                axes[0, col].set_title(f'{location_title} Congestion\\nRÂ² = {r2:.3f}')\n",
    "                axes[0, col].grid(True, alpha=0.3)\n",
    "                axes[0, col].legend()\n",
    "            else:\n",
    "                axes[0, col].text(0.5, 0.5, 'No Congestion Model', ha='center', va='center', transform=axes[0, col].transAxes)\n",
    "                axes[0, col].set_title(f'{location_title} Congestion\\nNo Model')\n",
    "\n",
    "            # 2. Demand Model Plot\n",
    "            if location in self.demand_models:\n",
    "                model_info = self.demand_models[location]\n",
    "                y_test = model_info['y_test']\n",
    "                y_pred = model_info['y_pred']\n",
    "                r2 = model_info['metrics']['r2']\n",
    "\n",
    "                axes[1, col].scatter(y_test, y_pred, alpha=0.6, s=20, color='blue')\n",
    "                min_val, max_val = min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())\n",
    "                axes[1, col].plot([min_val, max_val], [min_val, max_val], 'k--', lw=2, label='Perfect Prediction')\n",
    "                axes[1, col].set_xlabel('Actual Demand (MW)')\n",
    "                axes[1, col].set_ylabel('Predicted Demand (MW)')\n",
    "                axes[1, col].set_title(f'{location_title} Demand\\nRÂ² = {r2:.3f}')\n",
    "                axes[1, col].grid(True, alpha=0.3)\n",
    "                axes[1, col].legend()\n",
    "            else:\n",
    "                axes[1, col].text(0.5, 0.5, 'No Demand Model', ha='center', va='center', transform=axes[1, col].transAxes)\n",
    "                axes[1, col].set_title(f'{location_title} Demand\\nNo Model')\n",
    "\n",
    "            # 3. Price Model Plot\n",
    "            if location in self.price_models:\n",
    "                model_info = self.price_models[location]\n",
    "                y_test = model_info['y_test']\n",
    "                y_pred = model_info['y_pred']\n",
    "                r2 = model_info['metrics']['r2']\n",
    "\n",
    "                axes[2, col].scatter(y_test, y_pred, alpha=0.6, s=20, color='green')\n",
    "                min_val, max_val = min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())\n",
    "                axes[2, col].plot([min_val, max_val], [min_val, max_val], 'k--', lw=2, label='Perfect Prediction')\n",
    "                axes[2, col].set_xlabel('Actual Price ($/MWh)')\n",
    "                axes[2, col].set_ylabel('Predicted Price ($/MWh)')\n",
    "                axes[2, col].set_title(f'{location_title} Price\\nRÂ² = {r2:.3f}')\n",
    "                axes[2, col].grid(True, alpha=0.3)\n",
    "                axes[2, col].legend()\n",
    "            else:\n",
    "                axes[2, col].text(0.5, 0.5, 'No Price Model', ha='center', va='center', transform=axes[2, col].transAxes)\n",
    "                axes[2, col].set_title(f'{location_title} Price\\nNo Model')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def create_feature_importance_comparison(self):\n",
    "        \"\"\"Create feature importance comparison across all models.\"\"\"\n",
    "        print(\"ðŸ“Š Creating Feature Importance Analysis...\")\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        fig.suptitle('Feature Importance Comparison Across All Models', fontsize=16, fontweight='bold')\n",
    "\n",
    "        # 1. Congestion Model Feature Importance\n",
    "        if self.congestion_models:\n",
    "            self._plot_feature_importance(self.congestion_models, axes[0], \"Congestion Models\")\n",
    "\n",
    "        # 2. Demand Model Feature Importance\n",
    "        if self.demand_models:\n",
    "            self._plot_feature_importance(self.demand_models, axes[1], \"Demand Models\")\n",
    "\n",
    "        # 3. Price Model Feature Importance\n",
    "        if self.price_models:\n",
    "            self._plot_feature_importance(self.price_models, axes[2], \"Price Models\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_feature_importance(self, models, ax, title):\n",
    "        \"\"\"Helper function to plot feature importance.\"\"\"\n",
    "        feature_importance_data = {}\n",
    "\n",
    "        for location, model_info in models.items():\n",
    "            if not hasattr(model_info['model'], 'feature_importances_'):\n",
    "                continue\n",
    "\n",
    "            importances = model_info['model'].feature_importances_\n",
    "            feature_names = model_info['feature_names']\n",
    "            feature_importance_data[location] = dict(zip(feature_names, importances))\n",
    "\n",
    "        if not feature_importance_data:\n",
    "            ax.text(0.5, 0.5, 'No feature importance data', ha='center', va='center', transform=ax.transAxes)\n",
    "            ax.set_title(title)\n",
    "            return\n",
    "\n",
    "        # Create DataFrame for easier plotting\n",
    "        importance_df = pd.DataFrame(feature_importance_data).fillna(0)\n",
    "\n",
    "        # Plot top 10 most important features\n",
    "        feature_means = importance_df.mean(axis=1).sort_values(ascending=False)\n",
    "        top_features = feature_means.head(10).index\n",
    "\n",
    "        importance_subset = importance_df.loc[top_features]\n",
    "\n",
    "        # Create grouped bar chart\n",
    "        x = np.arange(len(top_features))\n",
    "        width = 0.25\n",
    "        colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "\n",
    "        for i, location in enumerate(importance_subset.columns):\n",
    "            ax.bar(x + i*width, importance_subset[location], width,\n",
    "                   label=location, color=colors[i % len(colors)])\n",
    "\n",
    "        ax.set_xlabel('Features')\n",
    "        ax.set_ylabel('Importance Score')\n",
    "        ax.set_title(title)\n",
    "        ax.set_xticks(x + width)\n",
    "        ax.set_xticklabels(top_features, rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    def create_time_series_analysis(self):\n",
    "        \"\"\"Create time series analysis of all variables.\"\"\"\n",
    "        if self.merged_dataset.empty:\n",
    "            return\n",
    "\n",
    "        fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "        fig.suptitle('NYC Energy Market Time Series Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "        # 1. Congestion trends\n",
    "        monthly_congestion = self.merged_dataset.groupby(['location', 'year', 'month'])['RTD Zonal Congestion'].mean().reset_index()\n",
    "        monthly_congestion['date'] = pd.to_datetime(monthly_congestion[['year', 'month']].assign(day=1))\n",
    "\n",
    "        for location in monthly_congestion['location'].unique():\n",
    "            location_monthly = monthly_congestion[monthly_congestion['location'] == location]\n",
    "            axes[0].plot(location_monthly['date'], location_monthly['RTD Zonal Congestion'],\n",
    "                        label=location, linewidth=2, marker='o', markersize=4)\n",
    "\n",
    "        axes[0].set_title('Monthly Average Congestion Trends')\n",
    "        axes[0].set_ylabel('Congestion ($/MWh)')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "        # 2. Demand trends\n",
    "        if 'TWI Actual Load' in self.merged_dataset.columns:\n",
    "            monthly_demand = self.merged_dataset.groupby(['location', 'year', 'month'])['TWI Actual Load'].mean().reset_index()\n",
    "            monthly_demand['date'] = pd.to_datetime(monthly_demand[['year', 'month']].assign(day=1))\n",
    "\n",
    "            for location in monthly_demand['location'].unique():\n",
    "                location_monthly = monthly_demand[monthly_demand['location'] == location]\n",
    "                axes[1].plot(location_monthly['date'], location_monthly['TWI Actual Load'],\n",
    "                            label=location, linewidth=2, marker='s', markersize=4)\n",
    "\n",
    "            axes[1].set_title('Monthly Average Demand Trends')\n",
    "            axes[1].set_ylabel('Demand (MW)')\n",
    "            axes[1].legend()\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "        # 3. Price trends\n",
    "        monthly_price = self.merged_dataset.groupby(['location', 'year', 'month'])['RTD Zonal LBMP'].mean().reset_index()\n",
    "        monthly_price['date'] = pd.to_datetime(monthly_price[['year', 'month']].assign(day=1))\n",
    "\n",
    "        for location in monthly_price['location'].unique():\n",
    "            location_monthly = monthly_price[monthly_price['location'] == location]\n",
    "            axes[2].plot(location_monthly['date'], location_monthly['RTD Zonal LBMP'],\n",
    "                        label=location, linewidth=2, marker='^', markersize=4)\n",
    "\n",
    "        axes[2].set_title('Monthly Average Price Trends')\n",
    "        axes[2].set_xlabel('Date')\n",
    "        axes[2].set_ylabel('Price ($/MWh)')\n",
    "        axes[2].legend()\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def generate_analysis_report(self):\n",
    "        \"\"\"Generate comprehensive analysis report.\"\"\"\n",
    "        print(\"\\nðŸ“‹ NYC ENERGY MARKET ANALYSIS REPORT\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print(f\"Report Generated: {current_time}\")\n",
    "\n",
    "        # Data Summary\n",
    "        print(f\"\\nðŸ“Š DATA OVERVIEW:\")\n",
    "        print(f\"   Total Records: {len(self.merged_dataset):,}\")\n",
    "        if 'location' in self.merged_dataset.columns:\n",
    "            for location in self.merged_dataset['location'].unique():\n",
    "                count = len(self.merged_dataset[self.merged_dataset['location'] == location])\n",
    "                print(f\"   {location}: {count:,} records\")\n",
    "\n",
    "        # Date Range\n",
    "        if 'RTD End Time Stamp' in self.merged_dataset.columns:\n",
    "            min_date = self.merged_dataset['RTD End Time Stamp'].min()\n",
    "            max_date = self.merged_dataset['RTD End Time Stamp'].max()\n",
    "            print(f\"   Date Range: {min_date.date()} to {max_date.date()}\")\n",
    "\n",
    "        # Model Performance Summary\n",
    "        print(f\"\\nðŸŽ¯ MODEL PERFORMANCE SUMMARY:\")\n",
    "\n",
    "        # Congestion Models\n",
    "        print(f\"\\n   ðŸ“Š CONGESTION MODELS:\")\n",
    "        for location, model_info in self.congestion_models.items():\n",
    "            metrics = model_info['metrics']\n",
    "            model_type = model_info['model_type']\n",
    "            location_display = \"All Regions Combined\" if location == 'combined' else location\n",
    "            print(f\"     {location_display} ({model_type}): RÂ²={metrics['r2']:.4f}, RMSE={metrics['rmse']:.4f}\")\n",
    "\n",
    "        # Demand Models\n",
    "        print(f\"\\n   âš¡ DEMAND MODELS (No Pricing/Congestion):\")\n",
    "        for location, model_info in self.demand_models.items():\n",
    "            metrics = model_info['metrics']\n",
    "            model_type = model_info['model_type']\n",
    "            location_display = \"All Regions Combined\" if location == 'combined' else location\n",
    "            print(f\"     {location_display} ({model_type}): RÂ²={metrics['r2']:.4f}, RMSE={metrics['rmse']:.2f} MW\")\n",
    "\n",
    "        # Price Models\n",
    "        print(f\"\\n   ðŸ’° PRICE MODELS (No Congestion):\")\n",
    "        for location, model_info in self.price_models.items():\n",
    "            metrics = model_info['metrics']\n",
    "            model_type = model_info['model_type']\n",
    "            location_display = \"All Regions Combined\" if location == 'combined' else location\n",
    "            print(f\"     {location_display} ({model_type}): RÂ²={metrics['r2']:.4f}, RMSE=${metrics['rmse']:.2f}/MWh\")\n",
    "\n",
    "        # Key Insights\n",
    "        print(f\"\\nðŸ” KEY INSIGHTS:\")\n",
    "\n",
    "        if not self.merged_dataset.empty:\n",
    "            # Best performing models (including combined)\n",
    "            if self.congestion_models:\n",
    "                congestion_r2 = {loc: info['metrics']['r2'] for loc, info in self.congestion_models.items()}\n",
    "                best_congestion = max(congestion_r2, key=congestion_r2.get)\n",
    "                best_congestion_display = \"All Regions Combined\" if best_congestion == 'combined' else best_congestion\n",
    "                print(f\"   Best Congestion Model: {best_congestion_display} (RÂ² = {congestion_r2[best_congestion]:.4f})\")\n",
    "\n",
    "            if self.demand_models:\n",
    "                demand_r2 = {loc: info['metrics']['r2'] for loc, info in self.demand_models.items()}\n",
    "                best_demand = max(demand_r2, key=demand_r2.get)\n",
    "                best_demand_display = \"All Regions Combined\" if best_demand == 'combined' else best_demand\n",
    "                print(f\"   Best Demand Model: {best_demand_display} (RÂ² = {demand_r2[best_demand]:.4f})\")\n",
    "\n",
    "            if self.price_models:\n",
    "                price_r2 = {loc: info['metrics']['r2'] for loc, info in self.price_models.items()}\n",
    "                best_price = max(price_r2, key=price_r2.get)\n",
    "                best_price_display = \"All Regions Combined\" if best_price == 'combined' else best_price\n",
    "                print(f\"   Best Price Model: {best_price_display} (RÂ² = {price_r2[best_price]:.4f})\")\n",
    "\n",
    "        print(f\"\\nâœ… Analysis Complete - Procedure Compliant!\")\n",
    "        print(f\"   âœ… Congestion Model: Uses RTD Zonal Congestion as y\")\n",
    "        print(f\"   âœ… Demand Model: Uses TWI Actual Load as y (No pricing/congestion features)\")\n",
    "        print(f\"   âœ… Price Model: Uses RTD Zonal LBMP as y (No congestion features)\")\n",
    "        print(f\"   âœ… Combined Models: Trained on all regions with location features\")\n",
    "\n",
    "    def run_complete_analysis(self):\n",
    "        \"\"\"Run the complete NYC energy analysis.\"\"\"\n",
    "        print(\"ðŸ—½ NYC ENERGY DATA ANALYSIS - PROCEDURE COMPLIANT\")\n",
    "        print(\"=\" * 65)\n",
    "\n",
    "        try:\n",
    "            # Load all data\n",
    "            self.load_all_merged_files()\n",
    "\n",
    "            # Show data summary\n",
    "            self.show_data_summary()\n",
    "\n",
    "            # Merge and prepare data\n",
    "            self.merge_all_data()\n",
    "\n",
    "            if self.merged_dataset.empty:\n",
    "                print(\"âŒ No merged data available for analysis!\")\n",
    "                return\n",
    "\n",
    "            # Train all models\n",
    "            self.train_all_models()\n",
    "\n",
    "            # Create visualizations\n",
    "            self.create_comprehensive_visualizations()\n",
    "\n",
    "            # Generate report\n",
    "            self.generate_analysis_report()\n",
    "\n",
    "            print(f\"\\nðŸŽ‰ Complete NYC Energy Analysis Finished!\")\n",
    "\n",
    "            return {\n",
    "                'congestion_models': self.congestion_models,\n",
    "                'demand_models': self.demand_models,\n",
    "                'price_models': self.price_models,\n",
    "                'merged_dataset': self.merged_dataset\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error in analysis: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the NYC energy analysis.\"\"\"\n",
    "\n",
    "    # Initialize analyzer\n",
    "    analyzer = NYCEnergyAnalyzer(data_directory=\".\")\n",
    "\n",
    "    # Run complete analysis\n",
    "    results = analyzer.run_complete_analysis()\n",
    "\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
